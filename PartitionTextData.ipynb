{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all entries spread across FlexiconCLEAN and Flexicon_NewData that are uncertain\n",
    "# entries originating from texts (these are notoriously messy and contain a lot of\n",
    "# duplicates and redundancies)\n",
    "# remove from FlexiconCLEAN.csv, new_data_matches.CSV, new_entries_matchesNEW.csv, and\n",
    "# Flexicon_NewData.csv\n",
    "\n",
    "# header\n",
    "import pandas as pd\n",
    "from GenerateLexDir import literal_eval_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dfs\n",
    "flexicon = pd.read_csv('flexiconCLEAN.csv', index_col='entry_id', keep_default_na=False)\n",
    "new_data = pd.read_csv('Flexport_2_1/Flexicon_NewData.csv', index_col='entry_id', keep_default_na=False)\n",
    "new_matches  = pd.read_csv('new_entries_matchesNEW.csv', index_col='entry_id', keep_default_na=False)\n",
    "edit_matches = pd.read_csv('new_data_matches.csv', index_col='entry_id', keep_default_na=False)\n",
    "senses = pd.read_csv('flex_senses.csv', index_col='sense_id', keep_default_na=False)\n",
    "new_senses = pd.read_csv('Flexport_2_1/Senses_NewData.csv', index_col='sense_id', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find cartilha entries\n",
    "# should be easy, as all entries will have 'cartilha' or 'lição/lições' somewhere in the\n",
    "# note or variant column\n",
    "# create list of entry_ids so we can remove it from all df's easily\n",
    "\n",
    "# don't call literal_eval yet so we can easily search thru row data as strings\n",
    "cartilha_idcs = set()\n",
    "cartilha_df = pd.DataFrame(columns = ['headword', 'morph_type', 'pronunciation',\\\n",
    "'variant_of', 'these_vars', 'note', 'sense', 'date', 'date_modified', 'other_sources'])\n",
    "\n",
    "for index, row in flexicon.copy().iterrows():\n",
    "    if 'cartilha' in row['note'] or 'liç' in row['note']\\\n",
    "    or 'cartilha' in row['variant_of'] or 'liç' in row['variant_of']:\n",
    "        cartilha_idcs.add(index)\n",
    "        flexicon = flexicon.drop(index)\n",
    "        row['date_modified'] = row['date']\n",
    "        cartilha_df.loc[index] = row\n",
    "\n",
    "for index, row in new_data.iterrows():\n",
    "    if 'cartilha' in row['note'] or 'liç' in row['note']\\\n",
    "    or 'cartilha' in row['variant_of'] or 'liç' in row['variant_of']:\n",
    "        cartilha_idcs.add(index)\n",
    "        new_data = new_data.drop(index)\n",
    "        row['other_sources'] = {}\n",
    "        cartilha_df.loc[index] = row\n",
    "\n",
    "len(cartilha_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['6e8e07f3-28a9-4adb-b230-f819c944b12a',\n",
       "       '1a7f5cab-b082-42eb-8cac-7781db2f1151',\n",
       "       '9e409799-d500-44e1-97b4-66925497edde',\n",
       "       '3954f8c6-283b-4e9f-a9b1-5e9ea6096fcc',\n",
       "       'a8675526-039d-459c-b4c8-c3434645200d',\n",
       "       '76dbc6d9-b69a-428f-8404-501dd5aa7637',\n",
       "       'baa0255c-d216-40c6-9314-c2b8ae598c65',\n",
       "       '9590b51f-1870-494d-872b-0b8ee8932571',\n",
       "       '93710c4c-822f-4475-9c65-0f1886aa7db0',\n",
       "       'b76b93ca-1032-4a40-9a95-1031ce3ca8d9',\n",
       "       ...\n",
       "       '9efb291e-c7ac-420a-8553-1d28627f81ec',\n",
       "       '9459cf68-19cc-45a7-8abd-b9d5d0d37887',\n",
       "       '3cf869fa-9f12-4d60-af32-0530c32fce8a',\n",
       "       '1ea498e1-3443-461b-aa55-d2d5b01ba48f',\n",
       "       '268369b7-3c64-46c9-8966-1b427339bd10',\n",
       "       'acd92b03-d069-4843-ae87-ad2288915bce',\n",
       "       '4713b0d1-e327-4b22-a41b-7db7430fe70e',\n",
       "       'b0db2efd-e199-42a1-969e-94d3131b1479',\n",
       "       'cb2b1a3f-9d04-468c-a497-8395bd836a75',\n",
       "       '34fe7ac1-e02e-46ae-b874-ca2c7cf8a244'],\n",
       "      dtype='object', name='sense_id', length=321)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next step is to identify entries Emily identified from texts\n",
    "# entries w/ a question mark in the PoS (or no PoS) are one\n",
    "no_pos = [(not x) or ('?' in x) for x in new_senses['pos']]\n",
    "\n",
    "# ditto for gloss and definition\n",
    "# however, since often only one of gloss and definition\n",
    "# fields contain data, concatenating both strs before\n",
    "# checking\n",
    "\n",
    "def_and_gloss = [d+g for d, g in zip(new_senses['gloss'], new_senses['def'])]\n",
    "no_def = [(not x) or ('?' in x) for x in def_and_gloss]\n",
    "\n",
    "text_senses = [x or y for x, y in zip(no_pos, no_def)]\n",
    "text_senses = new_senses.index[text_senses]\n",
    "text_senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find entries in new_data originating from texts\n",
    "# using text_senses to identify them\n",
    "text_entries = [any(t_s in x for t_s in text_senses) for x in new_data['sense']]\n",
    "keep = [not x for x in text_entries]\n",
    "text_entries = new_data[text_entries]\n",
    "new_data = new_data[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cover our steps- in theory, the tests we used to find text data from the recent import\n",
    "# should return no results for flexiconCLEAN - since we already segmented out Cartilha\n",
    "# data\n",
    "# let's make sure\n",
    "no_pos = [(not x) or ('?' in x) for x in senses['pos']]\n",
    "no_pos.count(True)\n",
    "\n",
    "def_and_gloss = [d+g for d, g in zip(senses['gloss'], senses['def'])]\n",
    "no_def = [(not x) or ('?' in x) for x in def_and_gloss]\n",
    "            \n",
    "text_senses = [x or y for x, y in zip(no_pos, no_def)]\n",
    "text_senses = senses.index[text_senses]\n",
    "len(text_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexicon_text_entries = [any(t_s in x for t_s in text_senses) for x in flexicon['sense']]\n",
    "keep = [not x for x in flexicon_text_entries]\n",
    "flexicon_text_entries = flexicon[flexicon_text_entries]\n",
    "flexicon = flexicon[keep]\n",
    "flexicon_text_entries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(534, 11)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# well, that didn't go as expected.\n",
    "# might as well aggregate all three df's into one\n",
    "# and write to csv\n",
    "\n",
    "text_entries.loc[:,'other_sources'] = [{} for i in text_entries.iterrows()]\n",
    "flexicon_text_entries.loc[:, 'date_modified'] = flexicon_text_entries['date']\n",
    "\n",
    "all_text_entries = pd.concat( [cartilha_df, flexicon_text_entries, text_entries] )\n",
    "all_text_entries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_entries.to_csv('all_text_entries.csv')\n",
    "new_data.to_csv('Flexport_2_1/Flexicon_NewDataPART.csv')\n",
    "flexicon.to_csv('flexiconPART.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
